{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T17:45:57.572037Z",
     "start_time": "2019-05-01T17:45:57.564328Z"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gluonnlp as nlp\n",
    "from utils.preprocessing import one_hot_encoder\n",
    "from utils.preprocessing import missing_values_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T05:58:17.368374Z",
     "start_time": "2019-05-01T05:57:03.100913Z"
    }
   },
   "outputs": [],
   "source": [
    "spec_feature = pd.read_csv('/Applications/files/classes_homework/Berkeley_ieor/data-x/project/feature_matrix_spec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:18.196813Z",
     "start_time": "2019-05-01T05:58:17.376623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 885 columns.\n",
      "823 of them have missing values.\n"
     ]
    }
   ],
   "source": [
    "missing_table = missing_values_table(spec_feature)\n",
    "\n",
    "dump_feats = missing_table[missing_table['% of Total Values'] > 30].index.tolist()\n",
    "\n",
    "spec_feature = spec_feature.drop(dump_feats, axis = 1)\n",
    "\n",
    "spec_feature, _ = one_hot_encoder(spec_feature)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "Target = spec_feature['TARGET']\n",
    "ID = spec_feature['SK_ID_CURR']\n",
    "\n",
    "dataset_temp = spec_feature.drop(['TARGET','SK_ID_CURR'], axis = 1)\n",
    "\n",
    "imputer = Imputer(strategy = 'median')\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "dataset_preprocessed = imputer.fit_transform(dataset_temp)\n",
    "dataset_preprocessed = scaler.fit_transform(dataset_preprocessed)\n",
    "dataset_preprocessed = pd.DataFrame(dataset_preprocessed, columns = dataset_temp.columns)\n",
    "\n",
    "dataset_preprocessed['TARGET'] = Target\n",
    "dataset_preprocessed['SK_ID_CURR'] = ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:20.539451Z",
     "start_time": "2019-05-01T06:01:18.217583Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_preprocessed = dataset_preprocessed[dataset_preprocessed.TARGET != -999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:22.406456Z",
     "start_time": "2019-05-01T06:01:20.542859Z"
    }
   },
   "outputs": [],
   "source": [
    "X_ = dataset_preprocessed.drop(['TARGET','SK_ID_CURR'], axis=1)\n",
    "y_ = dataset_preprocessed['TARGET']\n",
    "#print(X_.shape, y_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:29.702214Z",
     "start_time": "2019-05-01T06:01:22.409432Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(X_, y_, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:29.745210Z",
     "start_time": "2019-05-01T06:01:29.711851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215257, 789) (215257,)\n",
      "(92254, 789) (92254,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, train_labels.shape)\n",
    "\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:40.773114Z",
     "start_time": "2019-05-01T06:01:29.748065Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = nd.array(train_features)\n",
    "train_labels = nd.array(train_labels)\n",
    "test_features = nd.array(test_features)\n",
    "test_labels = nd.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:01:40.820604Z",
     "start_time": "2019-05-01T06:01:40.776763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215257, 789)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T18:47:27.226442Z",
     "start_time": "2019-05-01T18:47:27.199910Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_net(drop_prob1,drop_prob2):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(789, activation=\"relu\"),\n",
    "            # Add a dropout layer after the first fully connected layer\n",
    "            nn.Dropout(drop_prob1),\n",
    "            nn.Dense(128),\n",
    "            # Add a dropout layer after the second fully connected layer\n",
    "            nn.Dropout(drop_prob2),\n",
    "            nn.Dense(2)\n",
    "           )\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    return net\n",
    "net = get_net(0.1,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    " Use cross entropy as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T18:50:33.644032Z",
     "start_time": "2019-05-01T18:47:29.467385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3028, train acc 0.919, test acc 0.920\n",
      "epoch 2, loss 0.2820, train acc 0.919, test acc 0.920\n",
      "epoch 3, loss 0.2817, train acc 0.919, test acc 0.920\n",
      "epoch 4, loss 0.2817, train acc 0.919, test acc 0.920\n",
      "epoch 5, loss 0.2816, train acc 0.919, test acc 0.920\n",
      "epoch 6, loss 0.2816, train acc 0.919, test acc 0.920\n",
      "epoch 7, loss 0.2816, train acc 0.919, test acc 0.920\n",
      "epoch 8, loss 0.2816, train acc 0.919, test acc 0.920\n",
      "epoch 9, loss 0.2816, train acc 0.919, test acc 0.920\n",
      "epoch 10, loss 0.2815, train acc 0.919, test acc 0.920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01, 'wd':0.1})\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        train_features, train_labels), batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        test_features, test_labels), batch_size, shuffle=True)\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T18:51:13.667468Z",
     "start_time": "2019-05-01T18:51:11.474523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5374381970741873\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "preds = net(test_features).asnumpy()\n",
    "nn_auc = metrics.roc_auc_score(test_labels.asnumpy(),preds[:,1])\n",
    "print('AUC:',nn_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
