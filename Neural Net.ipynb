{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T17:58:57.614698Z",
     "start_time": "2019-04-22T17:58:53.007495Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephzeng/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gluonnlp as nlp\n",
    "from utils.preprocessing import one_hot_encoder\n",
    "from utils.preprocessing import missing_values_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T17:58:30.637710Z",
     "start_time": "2019-04-20T17:57:19.691348Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:00:04.404378Z",
     "start_time": "2019-04-22T17:58:57.617048Z"
    }
   },
   "outputs": [],
   "source": [
    "spec_feature = pd.read_csv('/Applications/files/classes_homework/Berkeley_ieor/data-x/project/feature_matrix_spec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:03:12.939039Z",
     "start_time": "2019-04-22T18:00:04.410881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 885 columns.\n",
      "823 of them have missing values.\n"
     ]
    }
   ],
   "source": [
    "missing_table = missing_values_table(spec_feature)\n",
    "\n",
    "dump_feats = missing_table[missing_table['% of Total Values'] > 30].index.tolist()\n",
    "\n",
    "spec_feature = spec_feature.drop(dump_feats, axis = 1)\n",
    "\n",
    "spec_feature, _ = one_hot_encoder(spec_feature)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "Target = spec_feature['TARGET']\n",
    "ID = spec_feature['SK_ID_CURR']\n",
    "\n",
    "dataset_temp = spec_feature.drop(['TARGET','SK_ID_CURR'], axis = 1)\n",
    "\n",
    "imputer = Imputer(strategy = 'median')\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "dataset_preprocessed = imputer.fit_transform(dataset_temp)\n",
    "dataset_preprocessed = scaler.fit_transform(dataset_preprocessed)\n",
    "dataset_preprocessed = pd.DataFrame(dataset_preprocessed, columns = dataset_temp.columns)\n",
    "\n",
    "dataset_preprocessed['TARGET'] = Target\n",
    "dataset_preprocessed['SK_ID_CURR'] = ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:05:38.148202Z",
     "start_time": "2019-04-22T18:05:34.635663Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_preprocessed = dataset_preprocessed[dataset_preprocessed.TARGET != -999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:07:52.949837Z",
     "start_time": "2019-04-22T18:07:48.381212Z"
    }
   },
   "outputs": [],
   "source": [
    "X_ = dataset_preprocessed.drop(['TARGET','SK_ID_CURR'], axis=1)\n",
    "y_ = dataset_preprocessed['TARGET']\n",
    "#print(X_.shape, y_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:08:03.654217Z",
     "start_time": "2019-04-22T18:07:56.229072Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(X_, y_, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-21T23:00:50.305Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_features.shape, train_labels.shape)\n",
    "\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:08:15.638225Z",
     "start_time": "2019-04-22T18:08:05.578245Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = nd.array(train_features)\n",
    "train_labels = nd.array(train_labels)\n",
    "test_features = nd.array(test_features)\n",
    "test_labels = nd.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:08:38.728431Z",
     "start_time": "2019-04-22T18:08:38.716238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215257, 789)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建神经网络（简单多层感知机）\n",
    "\n",
    "tensorflow和pytorch我都不太熟，就先用mxnet搭了个baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:39:24.969574Z",
     "start_time": "2019-04-22T19:39:24.962010Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_net(drop_prob1,drop_prob2):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(789, activation=\"relu\"),\n",
    "            # Add a dropout layer after the first fully connected layer\n",
    "            nn.Dropout(drop_prob1),\n",
    "            nn.Dense(64, activation=\"relu\"),\n",
    "            # Add a dropout layer after the second fully connected layer\n",
    "            nn.Dropout(drop_prob2),\n",
    "            nn.Dense(2)\n",
    "           )\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    return net\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:39:26.868800Z",
     "start_time": "2019-04-22T19:39:26.857772Z"
    }
   },
   "outputs": [],
   "source": [
    "net = get_net(0.2,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    " 这里以交叉熵作为metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:41:56.324393Z",
     "start_time": "2019-04-22T19:39:29.477771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3065, train acc 0.918, test acc 0.920\n",
      "epoch 2, loss 0.2844, train acc 0.919, test acc 0.920\n",
      "epoch 3, loss 0.2840, train acc 0.919, test acc 0.920\n",
      "epoch 4, loss 0.2838, train acc 0.919, test acc 0.920\n",
      "epoch 5, loss 0.2837, train acc 0.919, test acc 0.920\n",
      "epoch 6, loss 0.2835, train acc 0.919, test acc 0.920\n",
      "epoch 7, loss 0.2833, train acc 0.919, test acc 0.920\n",
      "epoch 8, loss 0.2833, train acc 0.919, test acc 0.920\n",
      "epoch 9, loss 0.2832, train acc 0.919, test acc 0.920\n",
      "epoch 10, loss 0.2832, train acc 0.919, test acc 0.920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03, 'wd':0.1})\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        train_features, train_labels), batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        test_features, test_labels), batch_size, shuffle=True)\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
